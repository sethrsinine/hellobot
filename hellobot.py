# èšå®¢AI
# è®²å¸ˆï¼škevin
import os
# ä¿®æ”¹äº†urllibæ–‡ä»¶å¤¹ä¸‹request çš„ProxySeverå‚æ•° å…·ä½“æ˜¯proxyServer: å¯ä»¥é€šè¿‡ctrl+F å®šä½åˆ°ä¿®æ”¹çš„ä½ç½®
import distutils.core


import streamlit as st
from openai import OpenAI

client = OpenAI(
    api_key="sk-fIRlLTNJO7EpNN1Z1dB3Af71144b450792Fb7cBe12Bd898d",
    base_url="https://free.gpt.ge/v1/",
    default_headers = {"x-foo": "true"}
)


# å¤§æ¨¡å‹ç»™å‡ºçš„ç­”æ¡ˆ
def chat_stream():
    # å‘é€æµå¼è¯·æ±‚ç»™ OpenAI æœåŠ¡å™¨
    response = client.chat.completions.create(
        # å…¬å¸å†…éƒ¨ï¼Œä¸€èˆ¬æ¥å…¥çš„æ˜¯è‡ªå·±çš„æ¨¡å‹ï¼ˆå‚ç›´æ¨¡å‹ã€è¡Œä¸šæ¨¡å‹ï¼‰
        model='gpt-3.5-turbo-16k',  # æ¥å…¥çš„å¤§æ¨¡å‹  gpt-4o mini ,æ¥å…¥ æœ€æ–°çš„äº§å“ èƒ½åŠ›
        messages=st.session_state.messages_history,
        temperature=0,
        stream=True  # è®¾ç½® stream=True
    )
    return response


def init_chat():
    st.session_state.messages = [{
        "role": "assistant",
        "content": "Hiï¼Œæˆ‘æ˜¯ Mobotï½ å¾ˆé«˜å…´é‡è§ä½ ï¼æœ‰é—®å¿…ç­”ï¼Œä¸“æ³¨äºæ‡‚ä½ çš„ AI ğŸ‘‹ "
    }]
    st.session_state.messages_history = [{
        "role": "system",
        "content": st.session_state.system_message
    }]

st.markdown("""
<style>.st-emotion-cache-1c7y2kd {flex-direction: row-reverse; text-align:right }</style>
""", unsafe_allow_html=True)

# å·¦ä¾§
with st.sidebar:
    # æ”¯æŒ markdown è¯­æ³•
    st.markdown(f"""
    <center>
    <img src='https://vip.helloimg.com/i/2024/07/02/66841f6f4a3a5.png' width='100'/>
    <h1> MoBot <sup>ğŸ’¬</sup><h1/>
    </center>
    """, unsafe_allow_html=True)

    # è§’è‰²å®šä¹‰è¾“å…¥æ¡† System Message
    system_message = st.text_area("è§’è‰²å®šä¹‰", "ä½ æ˜¯ä¸€ä¸ªèƒ½å¸®åŠ©ç”¨æˆ·çš„ AI åŠ©æ‰‹ã€‚", on_change=init_chat, key='system_message')

    # åˆ›é€ åŠ›è°ƒèŠ‚ Temperature
    temperature = st.slider("åˆ›é€ åŠ›è°ƒèŠ‚", min_value=0.0, max_value=2.0, value=1.0, step=0.1, help='å€¼è¶Šå¤§çº¦å…·æœ‰åˆ›é€ åŠ›',
                            format="%.1f")

    if st.button("ğŸ§¹ æ¸…é™¤èŠå¤©è®°å½•"):
        init_chat()


# å³ä¾§
st.title("Mobot AI èŠå¤©æœºå™¨äºº")

if "messages_history" not in st.session_state:
    st.session_state.messages_history = [
        {"role": "system", "content": system_message}
    ]

# åˆå§‹åŒ–ç•Œé¢çš„èŠå¤©åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "Hiï¼Œæˆ‘æ˜¯ Mobotï½ å¾ˆé«˜å…´é‡è§ä½ ï¼æœ‰é—®å¿…ç­”ï¼Œä¸“æ³¨äºæ‡‚ä½ çš„ AI ğŸ‘‹ "
        }
    ]

# æ˜¾ç¤ºå¯¹è¯çš„å†å²åˆ—è¡¨
for message in st.session_state.messages:
    # èŠå¤©çª—å£
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
user_query = st.chat_input("è¯´ç‚¹ä»€ä¹ˆ...")
if user_query:
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥çš„å†…å®¹åˆ°èŠå¤©çª—å£
    with st.chat_message("user"):
        st.write(user_query)
    # åœ¨èŠå¤©çª—å£è¾“å‡ºç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    st.session_state.messages.append(
        {
            "role": "user",
            "content": user_query
        }
    )
    st.session_state.messages_history.append({
        "role": "user",
        "content": user_query
    })

    with st.chat_message("assistant"):
        # è½¬åœˆç­‰å¾…
        with st.spinner(""):
            # AI çš„å›å¤ ï¼Œ æ¥å…¥äº† å¤§æ¨¡å‹
            response = chat_stream()
            # åˆ›å»ºæ˜¾ç¤ºæ¶ˆæ¯çš„å®¹å™¨
            message_placeholder = st.empty()
            # AI çš„ç­”æ¡ˆ
            ai_response = ""
            for chunk in response:
                # ä»æµå“åº”ä¸­è·å¾—AIçš„ç­”æ¡ˆ
                if chunk.choices and chunk.choices[0].delta.content:
                    ai_response += chunk.choices[0].delta.content
                    # æ˜¾ç¤ºAIçš„ç­”æ¡ˆ
                    message_placeholder.markdown(ai_response + "â–Œ")

            # åœ¨èŠå¤©çª—å£è¾“å‡ºå®Œæ•´çš„ AI ç­”æ¡ˆ
            message_placeholder.markdown(ai_response)
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": ai_response
                }
            )
            st.session_state.messages_history.append({
                "role": "assistant",
                "content": ai_response
            })
